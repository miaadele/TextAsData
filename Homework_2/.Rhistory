word_comp_tbl <- word_counts_normalized %>%
select(doc_title, word, n) %>%
pivot_wider(
names_from = doc_title,
values_from = n,
values_fill = 0
) %>%
mutate(max_n = pmax(`Text A`, `Text B`)) %>%
arrange(desc(max_n))
word_plot_data <- word_comp_tbl %>%
slice_head(n = plot_n_words) %>%
pivot_longer(
cols = c(`Text A`, `Text B`),
names_to = "doc_title",
values_to = "n"
) %>%
left_join(doc_lengths, by = "doc_title") %>%
mutate(
relative_freq = n / total_words,
word = fct_reorder(word, n, .fun = max)
)
ggplot(word_plot_data, aes(x = relative_freq, y = word)) +
geom_col() +
facet_wrap(~ doc_title, scales = "free_x") +
labs(
title = "Most relatively frequent words (stopwords removed)",
subtitle = paste0(
"Top ", plot_n_words,
" words by relative frequency across both texts"
),
x = "Relative frequency of word",
y = NULL
) +
theme_minimal()
#Homework Assignment: Sentiment Analysis and TF-IDF
#This code computes sentiment twice for the same texts.
#Sentiment is computed using 1) raw word counts and 2) TF-IDF weighted words
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
library(quanteda)
library(quanteda.textstats)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
#STEP 3: Compute raw sentiment totals
raw_sentiment <- bing %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
group_by(doc_title) %>%
summarise(
total_pos = sum(positive),
total_neg = sum(negative),
net_sentiment = total_pos - total_neg
)
raw_sentiment
#STEP 4: Compute TF-IDF for words in each doc
tfidf <- word_counts %>%
left_join(doc_lenghts) %>%
bind_tf_idf(word, doc_title, n)
tfidf
#STEP 5: Keep only sentiment-bearing words
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing")) %>%
arrange(desc(tf_idf)) #sort by highest tf-idf sentiment
tf_idf_bing
top_impact_words <- slice_head(tf_idf_bing, n = 5)
top_impact_words
#STEP 6: Compute TF-IDF-weighted sentiment totals
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
net_sentiment_tfidf = tfidf_positive - tfidf_negative
)
tfidf_weighted_summary
#STEP 7: Compare Raw vs. TF-IDF Sentiment
comp_tbl <- raw_sentiment %>%
left_join(tfidf_weighted_summary)
comp_tbl
#export table as CSV
write.csv(comp_tbl, file = 'HW2_final_table_Lassiter.csv')
View(word_counts)
View(tfidf_weighted_summary)
View(top_impact_words)
View(tfidf)
View(tf_idf_bing)
View(raw_sentiment)
View(bing)
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing")) %>%
arrange(desc(tf_idf)) #sort by highest tf-idf sentiment
tf_idf_bing
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing")) %>%
arrange(desc(tf_idf)) %>% #sort by highest tf-idf sentiment
pivot_wider(names_from = word, values_from = n)
tf_idf_bing
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing")) %>%
#arrange(desc(tf_idf)) %>% #sort by highest tf-idf sentiment
pivot_wider(names_from = word, values_from = n)
tf_idf_bing
View(bing)
View(raw_sentiment)
View(tf_idf_bing)
#Homework Assignment: Sentiment Analysis and TF-IDF
#This code computes sentiment twice for the same texts.
#Sentiment is computed using 1) raw word counts and 2) TF-IDF weighted words
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
library(quanteda)
library(quanteda.textstats)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
#STEP 3: Compute raw sentiment totals
raw_sentiment <- bing %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
group_by(doc_title) %>%
summarise(
total_pos = sum(positive),
total_neg = sum(negative),
net_sentiment = total_pos - total_neg
)
raw_sentiment
#STEP 4: Compute TF-IDF for words in each doc
tfidf <- word_counts %>%
left_join(doc_lenghts) %>%
bind_tf_idf(word, doc_title, n)
tfidf
#STEP 5: Keep only sentiment-bearing words
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing")) %>%
arrange(desc(tf_idf)) #sort by highest tf-idf sentiment
tf_idf_bing
top_impact_words <- slice_head(tf_idf_bing, n = 5)
top_impact_words
#STEP 6: Compute TF-IDF-weighted sentiment totals
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
net_sentiment_tfidf = tfidf_positive - tfidf_negative
)
tfidf_weighted_summary
#STEP 7: Compare Raw vs. TF-IDF Sentiment
comp_tbl <- raw_sentiment %>%
left_join(tfidf_weighted_summary)
comp_tbl
#export table as CSV
write.csv(comp_tbl, file = 'HW2_final_table_Lassiter.csv')
View(word_counts)
View(tf_idf_bing)
View(bing)
View(bing)
View(bing)
View(bing)
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
library(quanteda)
library(quanteda.textstats)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
bing
bing <- word_counts %>%
inner_join(get_sentiments("bing")) %>%
filter(n = 1)
bing <- word_counts %>%
inner_join(get_sentiments("bing")) %>%
filter(n == 1)
bing
unique_words <- bing %>%
group_by(word)
unique_words
unique_words <- bing %>%
group_by(word) %>%
filter(n_distinct(doc_title) == 1) %>%
ungroup()
unique_words
unique_cts <- unique_words %>%
count(doc_title, sentiment, name = "n")
unique_cts
unique_cts <- unique_words %>%
count(doc_title, sentiment, name = "n") %>%
pivot_wider(names_from = doc_title, values_from = n)
unique_cts
unique_cts <- unique_words %>%
count(doc_title, sentiment, name = "n") %>%
pivot_wider(names_from = sentiment, values_from = n)
unique_cts
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(forcats)
library(tibble)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing")) %>%
filter(n == 1)
bing
#filter OUT words that appear across both texts
unique_words <- bing %>%
group_by(word) %>%
filter(n_distinct(doc_title) == 1) %>%
ungroup()
unique_words
#Of the distinct words in each text, calculate the number of words with positive sentiment vs negative sentiment
unique_cts <- unique_words %>%
count(doc_title, sentiment, name = "n") %>%
pivot_wider(names_from = sentiment, values_from = n)
unique_cts
View(unique_words)
#This code filters distinct words in each doc
#This is not part of the homework assignment, but I calculated
#the number of positive vs negative distinct words are summed for each text
#in order to answer the analysis question
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(forcats)
library(tibble)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing")) %>%
bing
bing <- word_counts %>%
inner_join(get_sentiments("bing")) %>%
bing
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
bing
#filter OUT words that appear across both texts
unique_words <- bing %>%
group_by(word) %>%
filter(n_distinct(doc_title) == 1) %>%
ungroup()
unique_words
#This code filters distinct words in each doc
#This is not part of the homework assignment, but I calculated
#the number of positive vs negative distinct words are summed for each text
#in order to answer the analysis
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(forcats)
library(tibble)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
bing
#filter OUT words that appear across both texts
unique_words <- bing %>%
group_by(word) %>%
filter(n_distinct(doc_title) == 1) %>%
ungroup()
unique_words
#Of the distinct words in each text, calculate the number of words with positive sentiment vs negative sentiment
unique_cts <- unique_words %>%
count(doc_title, sentiment, name = "n") %>%
pivot_wider(names_from = sentiment, values_from = n)
unique_cts
