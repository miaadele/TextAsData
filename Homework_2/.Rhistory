texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
#STEP 3: Compute raw sentiment totals
raw_sentiment <- bing %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
group_by(doc_title) %>%
summarise(
total_pos = sum(positive),
total_neg = sum(negative),
net_sentiment = total_pos - total_neg
)
raw_sentiment
#STEP 4: Compute TF-IDF for words in each doc
tfidf <- word_counts %>%
left_join(doc_lenghts) %>%
bind_tf_idf(word, doc_title, n)
tfidf
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing"))
tf_idf_bing
#Homework Assignment: Sentiment Analysis and TF-IDF
#This code computes sentiment twice for the same texts.
#Sentiment is computed using 1) raw word counts and 2) TF-IDF weighted words
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
library(quanteda)
library(quanteda.textstats)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
#STEP 3: Compute raw sentiment totals
raw_sentiment <- bing %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
group_by(doc_title) %>%
summarise(
total_pos = sum(positive),
total_neg = sum(negative),
net_sentiment = total_pos - total_neg
)
raw_sentiment
#STEP 4: Compute TF-IDF for words in each doc
tfidf <- word_counts %>%
left_join(doc_lenghts) %>%
bind_tf_idf(word, doc_title, n)
tfidf
#STEP 5: Keep only sentiment-bearing words
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing"))
tf_idf_bing
#STEP 6: Compute TF-IDF-weighted sentiment totals
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
#replace NA values with 0
tdidf_positive = tidyr::replace_na(tfidf_positive, 0),
tfidf_negative = tidyr::replace_na(tfidf_negative, 0),
net_sent_tfidf = tfidf_positive - tfidf_negative
)
tfidf_weighted_summary
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
.groups = "drop"
)
tfidf_weighted_summary
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
net_tfidf = tfidf_positive - tfidf_negative
.groups = "drop"
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
net_sentiment_tfidf = tfidf_positive - tfidf_negative
)
tfidf_weighted_summary
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
#.groups = "drop"
) %>%
mutate(
net_sentiment_tfidf = tfidf_positive - tfidf_negative
)
tfidf_weighted_summary
#Homework Assignment: Sentiment Analysis and TF-IDF
#This code computes sentiment twice for the same texts.
#Sentiment is computed using 1) raw word counts and 2) TF-IDF weighted words
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
library(quanteda)
library(quanteda.textstats)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
#STEP 3: Compute raw sentiment totals
raw_sentiment <- bing %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
group_by(doc_title) %>%
summarise(
total_pos = sum(positive),
total_neg = sum(negative),
net_sentiment = total_pos - total_neg
)
raw_sentiment
#STEP 4: Compute TF-IDF for words in each doc
tfidf <- word_counts %>%
left_join(doc_lenghts) %>%
bind_tf_idf(word, doc_title, n)
tfidf
#STEP 5: Keep only sentiment-bearing words
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing"))
tf_idf_bing
#STEP 6: Compute TF-IDF-weighted sentiment totals
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
net_sentiment_tfidf = tfidf_positive - tfidf_negative
)
tfidf_weighted_summary
View(tfidf_weighted_summary)
View(tf_idf_bing)
View(raw_sentiment)
comp_tbl <- raw_sentiment %>%
left_join(tfidf_weighted_summary)
comp_tbl
#Homework Assignment: Sentiment Analysis and TF-IDF
#This code computes sentiment twice for the same texts.
#Sentiment is computed using 1) raw word counts and 2) TF-IDF weighted words
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
library(quanteda)
library(quanteda.textstats)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
#STEP 3: Compute raw sentiment totals
raw_sentiment <- bing %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
group_by(doc_title) %>%
summarise(
total_pos = sum(positive),
total_neg = sum(negative),
net_sentiment = total_pos - total_neg
)
raw_sentiment
#STEP 4: Compute TF-IDF for words in each doc
tfidf <- word_counts %>%
left_join(doc_lenghts) %>%
bind_tf_idf(word, doc_title, n)
tfidf
#STEP 5: Keep only sentiment-bearing words
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing"))
tf_idf_bing
#STEP 6: Compute TF-IDF-weighted sentiment totals
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
net_sentiment_tfidf = tfidf_positive - tfidf_negative
)
tfidf_weighted_summary
#STEP 7: Compare Raw vs. TF-IDF Sentiment
comp_tbl <- raw_sentiment %>%
left_join(tfidf_weighted_summary)
comp_tbl
View(comp_tbl)
#export table as CSV
write.csv(comp_tbl, file = 'HW2_final_table_Lassiter.csv')
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing")) %>%
arrange(desc(tf_idf))
tf_idf_bing
View(doc_lenghts)
#Homework Assignment: Sentiment Analysis and TF-IDF
#This code computes sentiment twice for the same texts.
#Sentiment is computed using 1) raw word counts and 2) TF-IDF weighted words
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
library(quanteda)
library(quanteda.textstats)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
#STEP 3: Compute raw sentiment totals
raw_sentiment <- bing %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
group_by(doc_title) %>%
summarise(
total_pos = sum(positive),
total_neg = sum(negative),
net_sentiment = total_pos - total_neg
)
raw_sentiment
#STEP 4: Compute TF-IDF for words in each doc
tfidf <- word_counts %>%
left_join(doc_lenghts) %>%
bind_tf_idf(word, doc_title, n)
tfidf
#STEP 5: Keep only sentiment-bearing words
tf_idf_bing <- tfidf %>%
inner_join(get_sentiments("bing")) %>%
arrange(desc(tf_idf)) #sort by highest tf-idf sentiment
tf_idf_bing
top_impact_words <- slice_head(tf_idf_bing, n = 5)
top_impact_words
#STEP 6: Compute TF-IDF-weighted sentiment totals
tfidf_weighted_summary <- tf_idf_bing %>%
group_by(doc_title) %>%
summarise(
tfidf_positive = sum(tf_idf[sentiment == "positive"], na.rm = TRUE),
tfidf_negative = sum(tf_idf[sentiment == "negative"], na.rm = TRUE),
.groups = "drop"
) %>%
mutate(
net_sentiment_tfidf = tfidf_positive - tfidf_negative
)
tfidf_weighted_summary
#STEP 7: Compare Raw vs. TF-IDF Sentiment
comp_tbl <- raw_sentiment %>%
left_join(tfidf_weighted_summary)
comp_tbl
#export table as CSV
write.csv(comp_tbl, file = 'HW2_final_table_Lassiter.csv')
View(top_impact_words)
View(comp_tbl)
View(raw_sentiment)
View(bing)
#Homework Assignment: Sentiment Analysis and TF-IDF
#This code computes sentiment twice for the same texts.
#Sentiment is computed using 1) raw word counts and 2) TF-IDF weighted words
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
library(forcats)
library(tibble)
library(scales)
library(quanteda)
library(quanteda.textstats)
file_a <- "texts/A07594__Circle_of_Commerce.txt"
file_b <- "texts/B14801__Free_Trade.txt"
text_a <- read_file(file_a)
text_b <- read_file(file_b)
#combine into a tibble for tidytext workflows
texts <- tibble(
doc_title = c("Text A", "Text B"),
text = c(text_a, text_b)
)
texts
#start with tidytext's build-in stopword list
data("stop_words")
#add project-specific stopwords
custom_stopwords <- tibble(
word = c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#STEP 1: tokenize and clean the text
word_counts <- texts %>%
unnest_tokens(word, text) %>% #automatically remove punctuation and lowercases
mutate(word = str_to_lower(word)) %>%
anti_join(all_stopwords, by = "word") %>%
count(doc_title, word, sort = TRUE)
#word_counts is a tidy table with the columns: doc_title, word, and n
#words in word_counts are lowercased
#stopwords have been removed
word_counts
#calculate total number of tokens in each doc (after lowercasing and stopword removal)
doc_lenghts <- word_counts %>%
group_by(doc_title) %>%
summarise(total_words = sum(n))
doc_lenghts
#STEP 2: Join tokens to the Bing sentiment dictionary
bing <- word_counts %>%
inner_join(get_sentiments("bing"))
bing
bing <- word_counts %>%
inner_join(get_sentiments("bing"), by = "word")
bing
