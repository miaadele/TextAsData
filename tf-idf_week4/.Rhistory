library(tidytext)
library(ggplot2)
library(forcats)
circle_raw <- read_file("texts/A07594__Circle_of_Commerce.txt")
text_tbl <- tibble(
doc_title = "The Circle of Commerce",
text = circle_raw
)
#nchar(text_tbl$text) #number of chars
#preserve original text in a column named text_original
text_tbl <- text_tbl %>%
mutate(text_original = text)
text_tbl <- text_tbl %>%
mutate(
text_clean = str_replace_all(text_original, "ſ", "s") #normalize the early modern ſ to the modern s
)
#sanity check
#tibble(
#  long_s_before = str_count(text_tbl$text_original, "ſ"),
#  long_s_after = str_count(text_tbl$text_clean, "ſ")
#)
text_tbl <- text_tbl %>%
mutate(
#remove all punctuation except for currency markers, apostrophes, and hyphens
text_clean = str_replace_all(text_clean, regex("[[:punct:]&&[^£'-]]"), " ")
)
#define a standardization map
name_map <- c(
"Smythe" = "Smith",
"Smyth" = "Smith",
"Smithe" = "Smith",
"traffick" = "traffic"
)
text_standard <- text_tbl %>%
mutate(
text_norm = text_clean %>%
str_replace_all(regex("\\bSmythe\\b", ignore_case = TRUE), "Smith")%>%
str_replace_all(regex("\\bSmyth\\b", ignore_case = TRUE), "Smith")%>%
str_replace_all(regex("\\bSmithe\\b", ignore_case = TRUE), "Smith") %>%
str_replace_all(regex("\\btraffick\\b", ignore_case = TRUE), "traffic")
)
#tokenize into bigrams
bigrams_raw <- text_standard %>%
select(doc_title, text_norm) %>%
unnest_tokens(output = "bigram", input = text_norm, token = "ngrams", n = 2)
bigrams_raw %>% count(bigram, sort = TRUE) %>% slice_head(n = 10)
#clean text_standard
data("stop_words")
bigrams_clean <- bigrams_raw %>%
separate(bigram, into = c("word1", "word2"), sep = " ") %>%
filter(!word1 %in% stop_words$word) %>%
filter(!word2 %in% stop_words$word) %>%
filter(str_detect(word1, "^[a-z]+$")) %>%
filter(str_detect(word2, "^[a-z]+$"))
bigrams_clean %>%
count(word1, word2, sort = TRUE) %>%
slice_head(n = 10)
#put bigrams back together
bigram_counts <- bigrams_clean %>%
count(word1, word2, sort = TRUE) %>%
unite("bigram", word1, word2, sep = " ")
#visualize bigram_counts
bigram_counts %>%
slice_head(n = 20) %>%
mutate(bigram = fct_reorder(bigram, n)) %>%
ggplot(aes(x = n, y = bigram)) + geom_col() + labs(
title = "Most frequent bigrams after stopword filtering",
x = "Count",
y = NULL
)
#filter bigrams with the token trade
trade_bigrams <- bigram_counts %>%
filter(str_detect(bigram, "\\btrade\\b"))
trade_bigrams %>% slice_head(n = 25)
trade_bigrams %>%
slice_head(n = 20) %>%
mutate(bigram = fct_reorder(bigram, n)) %>%
ggplot(aes(x = n, y = bigram)) + geom_col() + labs(
title = "Bigrams that include the word 'trade'",
x = "Count",
y = NULL
)
#create a trade lexicon
trade_lexicon <- c(
"trade", "traffick", "traffic", "commerce", "merchant", "merchants",
"exchange", "export", "import", "commodity", "commodities",
"navigation", "shipping", "market", "markets"
)
trade_theme_bigrams <- bigrams_clean %>%
filter(word1 %in% trade_lexicon | word2 %in% trade_lexicon) %>%
count(word1, word2, sort = TRUE) %>%
unite("bigram", word1, word2, sep = " ")
trade_theme_bigrams %>% slice_head(n =25)
trade_theme_bigrams %>%
slice_head(n = 20) %>%
mutate(bigram = fct_reorder(bigram, n)) %>%
ggplot(aes(x = n, y = bigram)) + geom_col() + labs(
title = "Trade-theme bigrams (lexicon-based)",
x = "Count",
y = NULL
)
#create a targeted sentiment analysis where we measure sentiment only in passages near “trade” (and related terms)
#this will give us more granular insight over the documents’ tone around the concept of trade
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
circle_raw <- read_file("texts/A07594__Circle_of_Commerce.txt")
free_raw   <- read_file("texts/B14801__Free_Trade.txt")
texts_miss <- tibble(
doc_title = c("Circle of Commerce", "Free Trade"),
text = c(circle_raw, free_raw)
)
#basic normalization
texts_miss <- texts_miss %>%
mutate(
text_norm = text %>%
str_replace_all("ſ", "s") %>%   # long s
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
#keep track of the position of each word in the document
tokens <- texts_miss %>%
unnest_tokens(word, text_norm, token = "words") %>%
group_by(doc_title) %>%
mutate(token_id = row_number()) %>%
ungroup()
tokens
#define a trade keyword set
#extract token windows around each keyword occurrence
#compute sentiment inside windows only
#compare both texts
#create a targeted sentiment analysis where we measure sentiment only in passages near “trade” (and related terms)
#this will give us more granular insight over the documents’ tone around the concept of trade
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
circle_raw <- read_file("texts/A07594__Circle_of_Commerce.txt")
free_raw   <- read_file("texts/B14801__Free_Trade.txt")
texts_miss <- tibble(
doc_title = c("Circle of Commerce", "Free Trade"),
text = c(circle_raw, free_raw)
)
#basic normalization
texts_miss <- texts_miss %>%
mutate(
text_norm = text %>%
str_replace_all("ſ", "s") %>%   # long s
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
#keep track of the position of each word in the document
tokens <- texts_miss %>%
unnest_tokens(word, text_norm, token = "words") %>%
group_by(doc_title) %>%
mutate(token_id = row_number()) %>%
ungroup()
tokens
#define a trade keyword set
trade_terms <- c("trade", "commerce", "merchant", "merchants")
#identify target words' locations
trade_hits <- tokens %>%
filter(word %in% trade_terms) %>%
select(doc_title, hit_word = word, hit_token_id = token_id)
#extract token windows around each keyword occurrence
window_size <- 30
trade_windows <- tokens %>%
inner_join(trade_hits, by = "doc_title") %>%
filter(token_id >= hit_token_id - window_size,
token_id <= hit_token_id + window_size) %>%
mutate(window_id = paste(doc_title, hit_token_id, sep = "_"))
#sanity check
trade_windows %>%
filter(window_id == nth(unique(window_id), 10)) %>%
summarise(window_text = str_c(word, collapse = " ")) %>%
pull(window_text) %>%
cat()
#compute sentiment inside windows only
#compare both texts
#create a targeted sentiment analysis where we measure sentiment only in passages near “trade” (and related terms)
#this will give us more granular insight over the documents’ tone around the concept of trade
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
circle_raw <- read_file("texts/A07594__Circle_of_Commerce.txt")
free_raw   <- read_file("texts/B14801__Free_Trade.txt")
texts_miss <- tibble(
doc_title = c("Circle of Commerce", "Free Trade"),
text = c(circle_raw, free_raw)
)
#basic normalization
texts_miss <- texts_miss %>%
mutate(
text_norm = text %>%
str_replace_all("ſ", "s") %>%   # long s
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
#keep track of the position of each word in the document
tokens <- texts_miss %>%
unnest_tokens(word, text_norm, token = "words") %>%
group_by(doc_title) %>%
mutate(token_id = row_number()) %>%
ungroup()
tokens
#define a trade keyword set
trade_terms <- c("trade", "commerce", "merchant", "merchants")
#identify target words' locations
trade_hits <- tokens %>%
filter(word %in% trade_terms) %>%
select(doc_title, hit_word = word, hit_token_id = token_id)
#extract token windows around each keyword occurrence
window_size <- 30
trade_windows <- tokens %>%
inner_join(trade_hits, by = "doc_title") %>%
filter(token_id >= hit_token_id - window_size,
token_id <= hit_token_id + window_size) %>%
mutate(window_id = paste(doc_title, hit_token_id, sep = "_"))
#sanity check
trade_windows %>%
filter(window_id == nth(unique(window_id), 10)) %>%
summarise(window_text = str_c(word, collapse = " ")) %>%
pull(window_text) %>%
cat()
#compute sentiment using bing lexicon inside windows only
bing <- get_sentiments("bing")
window_sentiment <- trade_windows %>%
inner_join(bing, by = "word") %>% #keep only sentiment-bearing words and label each token as poisitve or negative
count(doc_title, window_id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(net_sentiment = positive = negative)
#create a targeted sentiment analysis where we measure sentiment only in passages near “trade” (and related terms)
#this will give us more granular insight over the documents’ tone around the concept of trade
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
circle_raw <- read_file("texts/A07594__Circle_of_Commerce.txt")
free_raw   <- read_file("texts/B14801__Free_Trade.txt")
texts_miss <- tibble(
doc_title = c("Circle of Commerce", "Free Trade"),
text = c(circle_raw, free_raw)
)
#basic normalization
texts_miss <- texts_miss %>%
mutate(
text_norm = text %>%
str_replace_all("ſ", "s") %>%   # long s
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
#keep track of the position of each word in the document
tokens <- texts_miss %>%
unnest_tokens(word, text_norm, token = "words") %>%
group_by(doc_title) %>%
mutate(token_id = row_number()) %>%
ungroup()
tokens
#define a trade keyword set
trade_terms <- c("trade", "commerce", "merchant", "merchants")
#identify target words' locations
trade_hits <- tokens %>%
filter(word %in% trade_terms) %>%
select(doc_title, hit_word = word, hit_token_id = token_id)
#extract token windows around each keyword occurrence
window_size <- 30
trade_windows <- tokens %>%
inner_join(trade_hits, by = "doc_title") %>%
filter(token_id >= hit_token_id - window_size,
token_id <= hit_token_id + window_size) %>%
mutate(window_id = paste(doc_title, hit_token_id, sep = "_"))
#sanity check
trade_windows %>%
filter(window_id == nth(unique(window_id), 10)) %>%
summarise(window_text = str_c(word, collapse = " ")) %>%
pull(window_text) %>%
cat()
#compute sentiment using bing lexicon inside windows only
bing <- get_sentiments("bing")
window_sentiment <- trade_windows %>%
inner_join(bing, by = "word") %>% #keep only sentiment-bearing words and label each token as poisitve or negative
count(doc_title, window_id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(net_sentiment = positive - negative)
#summarize sentiments for each text
text_sentiment_summary <- window_sentiment %>%
group_by(doc_title) %>%
summarise(
windows = n(), #rows of window_sentiment
total_positive = sum(positive),
total_negative = sum(negative),
total_net_sentiment = sum(net_sentiment),
avg_net_per_window = mean(net_sentiment),
.groups = "drop"
)
text_sentiment_summary
#compare both texts
#create a targeted sentiment analysis where we measure sentiment only in passages near “trade” (and related terms)
#this will give us more granular insight over the documents’ tone around the concept of trade
library(readr)
library(dplyr)
library(tidyr)
library(stringr)
library(tidytext)
library(ggplot2)
circle_raw <- read_file("texts/A07594__Circle_of_Commerce.txt")
free_raw   <- read_file("texts/B14801__Free_Trade.txt")
texts_miss <- tibble(
doc_title = c("Circle of Commerce", "Free Trade"),
text = c(circle_raw, free_raw)
)
#basic normalization
texts_miss <- texts_miss %>%
mutate(
text_norm = text %>%
str_replace_all("ſ", "s") %>%   # long s
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
#keep track of the position of each word in the document
tokens <- texts_miss %>%
unnest_tokens(word, text_norm, token = "words") %>%
group_by(doc_title) %>%
mutate(token_id = row_number()) %>%
ungroup()
tokens
#define a trade keyword set
trade_terms <- c("trade", "commerce", "merchant", "merchants")
#identify target words' locations
trade_hits <- tokens %>%
filter(word %in% trade_terms) %>%
select(doc_title, hit_word = word, hit_token_id = token_id)
#extract token windows around each keyword occurrence
window_size <- 30
trade_windows <- tokens %>%
inner_join(trade_hits, by = "doc_title") %>%
filter(token_id >= hit_token_id - window_size,
token_id <= hit_token_id + window_size) %>%
mutate(window_id = paste(doc_title, hit_token_id, sep = "_"))
#sanity check
trade_windows %>%
filter(window_id == nth(unique(window_id), 10)) %>%
summarise(window_text = str_c(word, collapse = " ")) %>%
pull(window_text) %>%
cat()
#compute sentiment using bing lexicon inside windows only
bing <- get_sentiments("bing")
window_sentiment <- trade_windows %>%
inner_join(bing, by = "word") %>% #keep only sentiment-bearing words and label each token as poisitve or negative
count(doc_title, window_id, sentiment) %>%
pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) %>%
mutate(net_sentiment = positive - negative)
#summarize sentiments for each text
text_sentiment_summary <- window_sentiment %>%
group_by(doc_title) %>%
summarise(
windows = n(), #rows of window_sentiment
total_positive = sum(positive),
total_negative = sum(negative),
total_net_sentiment = sum(net_sentiment),
avg_net_per_window = mean(net_sentiment),
.groups = "drop"
)
text_sentiment_summary
#compare both texts
ggplot(window_sentiment, aes(x = net_sentiment)) +
geom_histogram(binwidth = 1) +
facet_wrap(~ doc_title, ncol = 1) +
labs(
title = "Sentiment in Trade-Centered Windows (±30 words)",
x = "Net sentiment per window",
y = "Number of trade windows"
)
setwd("~/Documents/GitHub/TextAsData/tf-idf_week4")
library(tibble)
library(dplyr)
library(readr)
library(ggplot2)
library(quanteda)
install.packages("quanteda")
library(tibble)
library(dplyr)
library(readr)
library(ggplot2)
library(quanteda)
library(quanteda.textstats)
library(quanteda.textstats)
dfm_mat
library(tibble)
library(dplyr)
library(readr)
library(ggplot2)
library(quanteda)
library(quanteda.textstats)
install.packages("quanteda.textstats")
library(tibble)
library(dplyr)
library(readr)
library(ggplot2)
library(quanteda)
library(quanteda.textstats)
txt_circle    <- read_file("texts/A07594__Circle_of_Commerce.txt")
txt_free      <- read_file("texts/B14801__Free_Trade.txt")
txt_third     <- read_file("texts/A06785.txt")
texts <- c(
"Circle_of_Commerce" = txt_circle,
"Free_Trade"         = txt_free,
"Third_Text_A06785"  = txt_third
)
corp <- corpus(texts)
#tokenization and basic cleaning
toks <- tokens(
corp,
remove_punct   = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE
)
toks <- tokens_tolower(toks)
custom_stop <- c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
toks <- tokens_remove(toks, pattern = c(stopwords("en"), custom_stop))
#build document-feature matrix
dfm_mat <- dfm(toks)
dfm_mat
topfeatures(dfm_mat, 25)
sim_cor <- textstat_simil(
dfm_mat,
method = "correlation",
margin = "documents"
)
sim_cor
#measure the cosine similarity
sim_cos <- textstat_simil(
dfm_mat,
method = "cosine",
margin = "documents"
)
sim_cos
dfm_tfidf <- dfm_tfidf(dfm_mat)
dfm_tfidf
topfeatures(dfm_tfidf, 20)
tfdif_mat <- as.matrix(dfm_tfidf)
circle_tfidf <- tfidf_mat["Circle_of_Commerce", ]
tfidf_mat <- as.matrix(dfm_tfidf)
#extract words with the top TF-IDF values for each text
circle_tfidf <- tfidf_mat["Circle_of_Commerce", ]
top_circle <- sort(circle_tfidf, decreasing = TRUE)[1:20]
top_circle
free_tfidf <- tfidf_mat["Free_Trade", ]
free_circle <- sort(free_tfidf, decreasing = TRUE)[1:20]
free_circle
mystery_tfidf <- tfidf_mat["Mystery_Text", ]
circle_tfidf <- tfidf_mat["Circle_of_Commerce", ]
top_circle <- sort(circle_tfidf, decreasing = TRUE)[1:20]
top_circle
free_tfidf <- tfidf_mat["Free_Trade", ]
free_circle <- sort(free_tfidf, decreasing = TRUE)[1:20]
free_circle
mystery_tfidf <- tfidf_mat["Mystery_Text", ]
circle_tfidf <- tfidf_mat["Circle_of_Commerce", ]
top_circle <- sort(circle_tfidf, decreasing = TRUE)[1:20]
top_circle
free_tfidf <- tfidf_mat["Free_Trade", ]
top_free<- sort(free_tfidf, decreasing = TRUE)[1:20]
top_free
mystery_tfidf <- tfidf_mat["Mystery_Text", ]
circle_tfidf <- tfidf_mat["Circle_of_Commerce", ]
top_circle <- sort(circle_tfidf, decreasing = TRUE)[1:20]
top_circle
free_tfidf <- tfidf_mat["Free_Trade", ]
top_free<- sort(free_tfidf, decreasing = TRUE)[1:20]
top_free
mystery_tfidf <- tfidf_mat["Mystery_Text", ]
A06785_tfidf <- tfidf_mat["Third_Text_A06785", ]
top_A06785 <- sort(A06785_tfidf, decreasing = TRUE)[1:20]
top_A06785
mystery_tfidf <- tfidf_mat["Mystery_Text", ]
mystery_tfidf <- tfidf_mat["Third_Text_A06785", ]
top_mystery <- sort(mystery_tfidf, decreasing = TRUE)[1:20]
top_mystery
tfidf_top_tbl <- bind_rows(
tibble(document = "Circle of Commerce",
term = names(top_circle),
tfidf = unname(top_circle)),
tibble(document = "Free Trade",
term = names(top_free),
tfidf = unname(top_free)),
tibble(document = "Third Text",
term = names(top_mystery),
tfidf = unname(top_mystery))
)
ggplot(tfidf_top_tbl, aes(x = tfidf, y = reorder(term, tfidf))) +
geom_col() +
facet_wrap(~ document, scales = "free_y") +
labs(
title = "Most Characteristic Terms by Document (TF–IDF)",
x = "TF–IDF score",
y = NULL
) +
theme_minimal()
