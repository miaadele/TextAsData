)
texts_tbl
#normalization and justifications:
#normalize the early modern ſ to the modern s
#collapse white space because historical texts can have irregular spaces due to archaic typography
#normalize all words to lower case because I want to analyze how often the texts talk about different tokens regardless of capitalization
texts_clean <- texts_tbl %>%
mutate(
text = text %>%
str_replace_all(text, "ſ", "s") %>%
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
View(texts_tbl)
library(dplyr)
library(tidyr)
library(tidytext)
library(tibble)
library(readr)
library(tidyverse)
library(readr)
library(purrr)
library(quanteda)
library(quanteda.textstats)
library(stringr)
#retrieve files
all_files <- list.files("texts/", full.names = TRUE)
all_files
#create cleaned title vector
doc_titles <- all_files |>
basename() |> #remove folder path
str_remove("\\.txt$")
doc_titles
#read each file into a tibble
texts_tbl <- map_dfr(
seq_along(all_files),
~ tibble(
doc_title = doc_titles[.x],
text = read_file(all_files[.x])
)
)
texts_tbl
#normalization and justifications:
#normalize the early modern ſ to the modern s
#collapse white space because historical texts can have irregular spaces due to archaic typography
#normalize all words to lower case because I want to analyze how often the texts talk about different tokens regardless of capitalization
texts_clean <- texts_tbl %>%
mutate(
text = text %>%
str_replace_all(text, "ſ", "s") %>%
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
texts_clean <- texts_tbl %>%
mutate(
text =
str_replace_all(text, "ſ", "s") %>%
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
texts_clean
View(texts_clean)
corp <- corpus(texts_tbl)
corp
corp <- corpus(texts_clean)
corp
toks <- tokens(
corp
)
toks
#remove stopwords
toks <- tokens_remove(
toks, pattern = all_stopwords
)
#create stopword list
data("stop_words")
custom_stopwords <- tibble(
word = c(
"vnto", "haue", "doo", "hath", "bee", "ye", "thee",
"hee", "shall", "hast", "doe", "beene", "thereof", "thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#tokenize
toks <- tokens(
corp
)
toks
#remove stopwords
toks <- tokens_remove(
toks, pattern = all_stopwords$word
)
toks
View(all_stopwords)
library(dplyr)
library(tidyr)
library(tidytext)
library(tibble)
library(readr)
library(tidyverse)
library(readr)
library(purrr)
library(quanteda)
library(quanteda.textstats)
library(stringr)
#retrieve files
all_files <- list.files("texts/", full.names = TRUE)
all_files
#create cleaned title vector
doc_titles <- all_files |>
basename() |> #remove folder path
str_remove("\\.txt$")
doc_titles
#read each file into a tibble
texts_tbl <- map_dfr(
seq_along(all_files),
~ tibble(
doc_title = doc_titles[.x],
text = read_file(all_files[.x])
)
)
texts_tbl
#normalization and justifications:
#normalize the early modern ſ to the modern s
#collapse white space because historical texts can have irregular spaces due to archaic typography
#normalize all words to lower case because I want to analyze how often the texts talk about different tokens regardless of capitalization
texts_clean <- texts_tbl %>%
mutate(
text =
str_replace_all(text, "ſ", "s") %>%
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
texts_clean
#combine texts into a quanteda corpus
corp <- corpus(texts_clean)
corp
#create stopword list
data("stop_words")
custom_stopwords <- tibble(
word = c(
"vnto", "haue", "doo", "hath", "bee", "ye", "thee",
"hee", "shall", "hast", "doe", "beene", "thereof", "thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#tokenize
#I am removing punctuation because I am not analyzing style
#I am removing numbers because pagination or rounded guesses may be distracting
#I am removing symbols because they are distracting, and
#the only meaningful ones are likely currency markers, which are
#no longer useful because numbers have been removed
toks <- tokens(
corp,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE
)
toks
#remove stopwords
#I am removing stopwords in order to decrease noise
toks <- tokens_remove(
toks, pattern = all_stopwords$word
)
toks
View(toks)
library(dplyr)
library(tidyr)
library(tidytext)
library(tibble)
library(readr)
library(tidyverse)
library(readr)
library(purrr)
library(quanteda)
library(quanteda.textstats)
library(stringr)
#retrieve files
all_files <- list.files("texts/", full.names = TRUE)
all_files
#create cleaned title vector
doc_titles <- all_files |>
basename() |> #remove folder path
str_remove("\\.txt$")
doc_titles
#read each file into a tibble
texts_tbl <- map_dfr(
seq_along(all_files),
~ tibble(
doc_title = doc_titles[.x],
text = read_file(all_files[.x])
)
)
texts_tbl
#normalization and justifications:
#normalize the early modern ſ to the modern s
#collapse white space because historical texts can have irregular spaces due to archaic typography
#normalize all words to lower case because I want to analyze how often the texts talk about different tokens regardless of capitalization
texts_clean <- texts_tbl %>%
mutate(
text =
str_replace_all(text, "ſ", "s") %>%
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
texts_clean
#combine texts into a quanteda corpus
corp <- corpus(texts_clean)
corp
#create stopword list
data("stop_words")
custom_stopwords <- tibble(
word = c(
"vnto", "haue", "doo", "hath", "bee", "ye", "thee",
"hee", "shall", "hast", "doe", "beene", "thereof", "thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#tokenize
#I am removing punctuation because I am not analyzing style
#I am removing numbers because pagination or rounded guesses may be distracting
#I am removing symbols because they are distracting, and
#the only meaningful ones are likely currency markers, which are
#no longer useful because numbers have been removed
toks <- tokens(
corp,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE
)
toks
#remove stopwords
#I am removing stopwords in order to decrease noise
toks <- tokens_remove(
toks, pattern = all_stopwords$word
)
toks
#save original text and clean text tibbles to CSV files
write_csv(texts_tbl, "original_texts.csv")
write_csv(texts_clean, "cleaned_texts.csv")
#save tokens to csv files
write_csv(toks, "tokens.csv")
write_csv(texts_clean, "cleaned_texts.csv")
#save original text and clean text tibbles to CSV files
write_csv(texts_tbl, "original_texts.csv")
write_csv(texts_clean, "cleaned_texts.csv")
View(stop_words)
View(texts_clean)
library(dplyr)
library(tidyr)
library(tidytext)
library(tibble)
library(readr)
library(tidyverse)
library(readr)
library(purrr)
library(quanteda)
library(quanteda.textstats)
library(stringr)
#retrieve files
all_files <- list.files("texts/", full.names = TRUE)
#all_files
#create cleaned title vector
doc_titles <- all_files |>
basename() |> #remove folder path
str_remove("\\.txt$")
#doc_titles
#read each file into a tibble
texts_tbl <- map_dfr(
seq_along(all_files),
~ tibble(
doc_title = doc_titles[.x],
text = read_file(all_files[.x])
)
)
#texts_tbl
#normalization and justifications:
#normalize the early modern ſ to the modern s
#collapse white space because historical texts can have irregular spaces due to archaic typography
#normalize all words to lower case because I want to analyze how often the texts talk about different tokens regardless of capitalization
texts_clean <- texts_tbl %>%
mutate(
text =
str_replace_all(text, "ſ", "s") %>%
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
#texts_clean
#combine texts into a quanteda corpus
corp <- corpus(texts_clean)
#corp
#create stopword list
data("stop_words")
custom_stopwords <- tibble(
word = c(
"vnto", "haue", "doo", "hath", "bee", "ye", "thee",
"hee", "shall", "hast", "doe", "beene", "thereof", "thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#tokenize
#I am removing punctuation because I am not analyzing style
#I am removing numbers because pagination or rounded guesses may be distracting
#I am removing symbols because they are distracting, and
#the only meaningful ones are likely currency markers, which are
#no longer useful because numbers have been removed
toks <- tokens(
corp,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE
)
#toks
#remove stopwords
#I am removing stopwords in order to decrease noise
toks <- tokens_remove(
toks, pattern = all_stopwords$word
)
toks
#--------------------------------------------------
#Approach 1: TF-IDF, lexical distinctiveness
#Step 1: Construct a DFM for the full corpus
df_mat <- dfm(toks)
df_mat
df_mat = dfm_trim(min_termfreq = 5)
df_trimmed = dfm_trim(df_mat, min_termfreq = 5)
df_trimmed
sim_cor <- textstat_simil(
df_trimmed,
method = "correlation",
margin = "documents"
)
sim_cor
write.csv(sim_cor, file = "CorrelationSimilarity")
write.csv(sim_cor, file = "CorrelationSimilarity.csv")
write.csv(sim_cor, "CorrelationSimilarity.csv")
write.csv(sim_cor, file = 'CorrelationSimilarity.csv')
class(sim_cor)
sim_cor_df <- as.data.frame(as.matrix(sim_cor))
write.csv(sim_cor_df, "correlation_similarity.csv")
class(sim_cor_df)
sim_cor_df <- as.data.frame(sim_cor)
write.csv(sim_cor_df, "correlation_similarity.csv")
sim_cor_mat <- as.matrix(sim_cor)
write.csv(sim_cor_mat, "correlation_similarity.csv")
library(dplyr)
library(tidyr)
library(tidytext)
library(tibble)
library(readr)
library(tidyverse)
library(readr)
library(purrr)
library(quanteda)
library(quanteda.textstats)
library(stringr)
#retrieve files
all_files <- list.files("texts/", full.names = TRUE)
#all_files
#create cleaned title vector
doc_titles <- all_files |>
basename() |> #remove folder path
str_remove("\\.txt$")
#doc_titles
#read each file into a tibble
texts_tbl <- map_dfr(
seq_along(all_files),
~ tibble(
doc_title = doc_titles[.x],
text = read_file(all_files[.x])
)
)
#texts_tbl
#normalization and justifications:
#normalize the early modern ſ to the modern s
#collapse white space because historical texts can have irregular spaces due to archaic typography
#normalize all words to lower case because I want to analyze how often the texts talk about different tokens regardless of capitalization
texts_clean <- texts_tbl %>%
mutate(
text =
str_replace_all(text, "ſ", "s") %>%
str_replace_all("\\s+", " ") %>% # collapse whitespace
str_to_lower()
)
#texts_clean
#combine texts into a quanteda corpus
corp <- corpus(texts_clean)
#corp
#create stopword list
data("stop_words")
custom_stopwords <- tibble(
word = c(
"vnto", "haue", "doo", "hath", "bee", "ye", "thee",
"hee", "shall", "hast", "doe", "beene", "thereof", "thus"
)
)
all_stopwords <- bind_rows(stop_words, custom_stopwords) %>%
distinct(word)
#tokenize
#I am removing punctuation because I am not analyzing style
#I am removing numbers because pagination or rounded guesses may be distracting
#I am removing symbols because they are distracting, and
#the only meaningful ones are likely currency markers, which are
#no longer useful because numbers have been removed
toks <- tokens(
corp,
remove_punct = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE
)
#toks
#remove stopwords
#I am removing stopwords in order to decrease noise
toks <- tokens_remove(
toks, pattern = all_stopwords$word
)
toks
df_mat <- dfm(toks)
#--------------------------------------------------
#Approach 1: TF-IDF, lexical distinctiveness
#Step 1: Construct a DFM for the full corpus
df_mat
#Step 2: Compute TF-IDF weights
#Step 3:For each document, extract the top 15 TF-IDF terms
#--------------------------------------------------
#Approach 2: Pearson correlation, similarity and distance between texts
#Step 1: Compute pairwise correlations between documents
#first, trim rare words (frequency of less than 5)
df_trimmed = dfm_trim(df_mat, min_termfreq = 5)
df_trimmed
#correlation similarity
sim_cor <- textstat_simil(
df_trimmed,
method = "correlation",
margin = "documents"
)
sim_cor
View(df_mat)
class(df_mat)
View(df_mat)
View(toks)
view(toks)
class(toks)
View(toks)
View(texts_tbl)
View(texts_clean)
setwd("~/GitHub/TextAsData/tf-idf_week4")
library(tibble)
library(dplyr)
library(readr)
library(ggplot2)
library(quanteda)
library(quanteda.textstats)
txt_circle    <- read_file("texts/A07594__Circle_of_Commerce.txt")
txt_free      <- read_file("texts/B14801__Free_Trade.txt")
txt_third     <- read_file("texts/A06785.txt")
texts <- c(
"Circle_of_Commerce" = txt_circle,
"Free_Trade"         = txt_free,
"Third_Text_A06785"  = txt_third
)
corp <- corpus(texts)
#tokenization and basic cleaning
toks <- tokens(
corp,
remove_punct   = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE
)
toks <- tokens_tolower(toks)
custom_stop <- c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
toks <- tokens_remove(toks, pattern = c(stopwords("en"), custom_stop))
#build document-feature matrix
dfm_mat <- dfm(toks)
dfm_mat
all_files <- list.files("texts/", full.names = TRUE)
all_files
txt_circle    <- read_file("texts/A07594__Circle_of_Commerce.txt")
txt_free      <- read_file("texts/B14801__Free_Trade.txt")
txt_third     <- read_file("texts/A06785.txt")
texts <- c(
"Circle_of_Commerce" = txt_circle,
"Free_Trade"         = txt_free,
"Third_Text_A06785"  = txt_third
)
corp <- corpus(texts)
toks <- tokens(
corp,
remove_punct   = TRUE,
remove_numbers = TRUE,
remove_symbols = TRUE
)
toks <- tokens_tolower(toks)
custom_stop <- c(
"vnto","haue","doo","hath","bee","ye","thee","hee","shall","hast","doe",
"beene","thereof","thus"
)
toks <- tokens_remove(toks, pattern = c(stopwords("en"), custom_stop))
#build document-feature matrix
dfm_mat <- dfm(toks)
dfm_mat
View(texts_clean)
type(texts)
view(texts)
class(texts)
