group_by(document, author) %>%
summarise(
tokens = n(), #count total words
types = n_distinct(word), #count unique words
ttr = types / tokens,
.groups = "drop"
)
#display results
ttr_results %>%
knitr::kable(
digits = 3,
caption = "TTR Results",
col.names = c(
"Document",
"Author",
"Total Words",
"Unique Words",
"TTR"
)
)
ggplot(ttr_results, aes(x = author, y = ttr, fill = author)) +
geom_col(width = 0.6) +
geom_text(aes(label = round(ttr, 3)),
vjust = -0.5, size = 5) +
scale_fill_brewer(palette = "Set2") +
labs(
title = "Type-Token Ratio Comparison",
subtitle = "Higher values = more diverse vocabulary",
x = NULL,
y = "TTR"
) +
theme(legend.position = "none") +
ylim(0, max(ttr_results$ttr) * 1.15)
#Calculate Guiraud Index for each text
guiraud_results <- tokens %>%
group_by(document, author) %>%
summarise(
tokens = n(),
types = n_distinct(word),
guiraud = types / sqrt(tokens),
.groups = "drop"
)
knitr::kable(
digits = 3,
caption = "Guiraud Index Results",
col.names = c(
"Document",
"Author",
"Total Words",
"Unique Words",
"Guiraud Index"
)
)
#Calculate Guiraud Index for each text
guiraud_results <- tokens %>%
group_by(document, author) %>%
summarise(
tokens = n(),
types = n_distinct(word),
guiraud = types / sqrt(tokens),
.groups = "drop"
)
guiraud_results %>%
knitr::kable(
digits = 3,
caption = "Guiraud Index Results",
col.names = c(
"Document",
"Author",
"Total Words",
"Unique Words",
"Guiraud Index"
)
)
ggplot(guiraud_results, aes(x = author, y = guiraud, fill = author)) +
geom_col(width = 0.6) +
geom_text(aes(label = round(guiraud, 2)),
vjust = -0.5, size = 5) +
scale_fill_brewer(pallete = "Set1") +
labs(
title = "Guiraud Index Comparison",
subtitle = "Length-corrected measure of lexical diversity",
x = NULL,
y = "Guiraud Index"
) +
theme(legend.position = "none") +
ylim(0, max(guiraud_results$guiraud) * 1.15)
ggplot(guiraud_results, aes(x = author, y = guiraud, fill = author)) +
geom_col(width = 0.6) +
geom_text(aes(label = round(guiraud, 2)),
vjust = -0.5, size = 5) +
scale_fill_brewer(palette = "Set1") +
labs(
title = "Guiraud Index Comparison",
subtitle = "Length-corrected measure of lexical diversity",
x = NULL,
y = "Guiraud Index"
) +
theme(legend.position = "none") +
ylim(0, max(guiraud_results$guiraud) * 1.15)
#Calculate MTLD for a text
calc_mtld <- function(text_string, docname) {
temp_file <- tempfile(fileext = ".txt")
writeLines(text_str, temp_file)
tokenized <- tokenize(temp_file, lang = "en") #tokenize with koRpus
mtld_result <- MTLD(tokenized) #calculate MTLD
mtld_val <- mtld_result@MTLD$MTLD
unlink(temp_file) #clean up
return(mtld_val)
}
#Calculate MTLD for both texts
mtld_results <- texts_df %>%
rowwise() %>%
mutate(
mtld = calculate_mtld(text, document)
) %>%
ungroup() %>%
select(document, author, mtld)
mtld_results <- texts_df %>%
rowwise() %>%
mutate(
mtld = calculate_mtld(text, document)
) %>%
ungroup() %>%
select(document, author, mtld)
calc_mtld <- function(text_string, docname) {
temp_file <- tempfile(fileext = ".txt")
writeLines(text_str, temp_file)
tokenized <- tokenize(temp_file, lang = "en") #tokenize with koRpus
mtld_result <- MTLD(tokenized) #calculate MTLD
mtld_val <- mtld_result@MTLD$MTLD
unlink(temp_file) #clean up
return(mtld_val)
}
#Calculate MTLD for both texts
mtld_results <- texts_df %>%
rowwise() %>%
mutate(
mtld = calc_mtld(text, document)
) %>%
ungroup() %>%
select(document, author, mtld)
calc_mtld <- function(text_string, docname) {
temp_file <- tempfile(fileext = ".txt")
writeLines(text_str, temp_file)
tokenized <- tokenize(temp_file, lang = "en") #tokenize with koRpus
mtld_result <- MTLD(tokenized) #calculate MTLD
mtld_val <- mtld_result@MTLD$MTLD
unlink(temp_file) #clean up
return(mtld_val)
}
#Calculate MTLD for both texts
mtld_results <- texts_df %>%
rowwise() %>%
mutate(
mtld = calc_mtld(text, document)
) %>%
ungroup() %>%
select(document, author, mtld)
calculate_mtld <- function(text_string, docname) {
temp_file <- tempfile(fileext = ".txt")
writeLines(text_str, temp_file)
tokenized <- tokenize(temp_file, lang = "en") #tokenize with koRpus
mtld_result <- MTLD(tokenized) #calculate MTLD
mtld_val <- mtld_result@MTLD$MTLD
unlink(temp_file) #clean up
return(mtld_val)
}
#Calculate MTLD for both texts
mtld_results <- texts_df %>%
rowwise() %>%
mutate(
mtld = calculate_mtld(text, document)
) %>%
ungroup() %>%
select(document, author, mtld)
calculate_mtld <- function(text_string, docname) {
temp_file <- tempfile(fileext = ".txt")
writeLines(text_string, temp_file)
tokenized <- tokenize(temp_file, lang = "en") #tokenize with koRpus
mtld_result <- MTLD(tokenized) #calculate MTLD
mtld_val <- mtld_result@MTLD$MTLD
unlink(temp_file) #clean up
return(mtld_val)
}
#Calculate MTLD for both texts
mtld_results <- texts_df %>%
rowwise() %>%
mutate(
mtld = calculate_mtld(text, document)
) %>%
ungroup() %>%
select(document, author, mtld)
#display results
mtld_results %>%
knitr::kable(
digits = 2,
caption = "MTLD Results",
col.names = c(
"Document",
"Author",
"MTLD"
)
)
ggplot(mtld_results, aes(x = author, y = mtld, fill = author)) +
geom_col(width = 0.6) +
geom_text(aes(label = round(mtld, 1)),
vjust = -0.5, size = 5) +
scale_fill_brewer(palette = "Dark2") +
labs(
title = "MTLD Comparison",
subtitle = "Mean Length of Sequential Word Strings (Higher = More Diverse)",
x = NULL,
y = "MTLD Score"
) +
theme(legend.position = "none") +
ylim(0, max(mtld_results$mtld) * 1.15)
library(readr)
library(tidyverse)
library(tidyr)
library(tidytext)
library(ggplot2)
library(udpipe)
library(koRpus)
library(koRpus.lang.en)
circle <- read_file("texts/A07594__Circle_of_Commerce.txt")
mystery <- read_file("texts/A69858.txt")
texts_df <- tibble(
document = c("Circle of Commerce", "A69858"),
author = c("Misselden", "Unknown"),
text = c(circle, mystery)
)
texts_df
#load an English universal dependency (UD) model once
model_info <- udpipe_download_model("language = english-ewt")
#load an English universal dependency (UD) model once
model_info <- udpipe_download_model(language = "english-ewt")
ud_model <- udpipe_load_model(model_info$file_model)
#annotate both texts using UDPipe
anno_df <- texts_df %>%
mutate(
#parse each text with UD parser, and set doc_id to our doc name
anno - map2(
text,
document,
~udpipe_annotate(
ud_model,
x = .x,
doc_id = .y
) %>%
as.data.frame()
)
) %>%
select(anno) %>% #keep only parsed annotations
unnest(anno) %>% #unnest into rows
rename(document = doc_id) %>% #use the UD doc_id as document label, and drop any duplicates cleanlu
select( #select columns for syntactic analysis
document,
paragraph_id,
sentence_id,
token_id,
token,
lemma,
upos, #part of speech
feats, #grammatical features
head_token_id, #head of dependency relation
dep_rel #dependency relation type
)
anno_df <- texts_df %>%
mutate(
#parse each text with UD parser, and set doc_id to our doc name
anno = map2(
text,
document,
~udpipe_annotate(
ud_model,
x = .x,
doc_id = .y
) %>%
as.data.frame()
)
) %>%
select(anno) %>% #keep only parsed annotations
unnest(anno) %>% #unnest into rows
rename(document = doc_id) %>% #use the UD doc_id as document label, and drop any duplicates cleanlu
select( #select columns for syntactic analysis
document,
paragraph_id,
sentence_id,
token_id,
token,
lemma,
upos, #part of speech
feats, #grammatical features
head_token_id, #head of dependency relation
dep_rel #dependency relation type
)
anno_df %>%
glimpse()
example_sentence <- tibble(
token = c("The", "big", "dog", "barks"),
token_id = c(1,2,3,4),
head_token_id = c(3,3,4,0),
Relationship = c(
'"The" depends on word #3 (dog)',
'"big" depends on word #3 (dog)',
'"dog" depends on word #4 (barks)',
'"barks" is the ROOT (doesn\'t depend on anything)'
)
)
example_sentence %>%
knitr::kable(
caption = 'Example: Dependency structure of "The big dog barks"',
align = c("l","c","c","l")
)
#create binary flags for different syntactic structures
syntax_df <- anno_df %>%
mutate(
#is it a word (i.e. not punctuation)?
is_word = upos != "PUNCT",
#is it an independent clause? Finite verbs are proxy for ind clauses
is_clause = (upos %in% c("VERB", "AUX")) & str_detect(coalesce(feats, ""), "VerbForm=Fin")
#is it a dependent clause?
is_dep_clause = dep_rel %in% c(
syntax_df <- anno_df %>%
mutate(
#is it a word (i.e. not punctuation)?
is_word = upos != "PUNCT",
#is it an independent clause? Finite verbs are proxy for ind clauses
is_clause = (upos %in% c("VERB", "AUX")) & str_detect(coalesce(feats, ""), "VerbForm=Fin"),
#is it a dependent clause?
is_dep_clause = dep_rel %in% c(
"advcl", #adverbial clause
"ccomp", #clausal complement
"xcomp", #open clausal complement
"acl", #adnomial clause
"acl:relcl" #relative clause
),
#is it coordination?
is_coord = dep_rel %in% c("conj", "cc"),
#nominal complexity
is_complex_nominal = dep_rel %in% c(
"amod", #adjective modifier
"nmod", #nominal modifier
"compound", #compoud
"appos" #apposition
)
)
syntax_df %>%
select(document, token, upos, is_clause, is_dep_clause) %>%
head(20)
sentence_df <= suntax_df %>%
filter(is_word) %>%
group_by(document, sentence_id) %>%
summarise(
words = n(), #number of words per sentence
clauses = sum(is_clause), #number of clauses per sentence
dep_clauses = sum(is_dep_clause), #number of dependent clauses per sentence
.groups = "drop"
)
sentence_df <= suntax_df %>%
filter(is_word) %>%
group_by(document, sentence_id) %>%
summarise(
words = n(), #number of words per sentence
clauses = sum(is_clause), #number of clauses per sentence
dep_clauses = sum(is_dep_clause), #number of dependent clauses per sentence
.groups = "drop"
)
sentence_df <- suntax_df %>%
filter(is_word) %>%
group_by(document, sentence_id) %>%
summarise(
words = n(), #number of words per sentence
clauses = sum(is_clause), #number of clauses per sentence
dep_clauses = sum(is_dep_clause), #number of dependent clauses per sentence
.groups = "drop"
)
sentence_df <- syntax_df %>%
filter(is_word) %>%
group_by(document, sentence_id) %>%
summarise(
words = n(), #number of words per sentence
clauses = sum(is_clause), #number of clauses per sentence
dep_clauses = sum(is_dep_clause), #number of dependent clauses per sentence
.groups = "drop"
)
sentence_df
mls_df <- sentence_df %>%
group_by(document) %>%
summarise(
MLS = mean(words),
.groups = "drop"
)
mls_df
clausal_density_df <- sentence_df %>%
group_by(document) %>%
summarise(
sentences = n(),
clauses = sum(clauses),
C_per_S = clauses / sentences,
.groups = "drop"
)
clausal_density_df
subordination_df <- sentence_df %>%
group_by(document) %>%
summarise(
clauses = sum(clauses),
dep_clauses = sum(dep_clauses),
sentences = n(),
DC_per_C = dep_clauses / pmax(clauses, 1), #avoid division by 0
DC_per_S = dep_clauses / sentences,
.groups = "drop"
)
subordination_df
coordination_df <- syntax_df %>%
group_by(document) %>%
summarise(
coord_relations = sum(is_coord),
clauses = sum(is_clause),
sentences = n_distinct(sentence_id),
Coord_per_C = coord_relations / pmax(clauses, 1),
Coord_per_S = coord_relations / sentences,
.groups = "drop"
)
coordination_df
nominal_df <- syntax_df %>%
group_by(document) %>%
summarise(
complex_nominals = sum(is_complex_nominal),
clauses = sum(is_clause),
sentences = n_distinct(sentence_id),
CN_per_C = complex_nominals / pmax(clauses, 1),
CN_per_S = complex_nominals / sentences,
.groups = "drop"
)
nominal_df
all_measures <- mls_df %>%
left_join(clausal_density_df %>%
select(document, C_per_S), by = "document") %>%
left_join(subordination_df %>%
select(document, DC_per_C, DC_per_S), by = "document") %>%
left_join(coordination_df %>%
select(document, Coord_per_C, Coord_per_S), by = "document") %>%
left_join(nominal_df %>%
select(document, CN_per_C, CN_per_S), by = "doocument")
all_measures <- mls_df %>%  # ← Added mls_df %>%
left_join(clausal_density_df %>% select(document, C_per_S), by = "document") %>%
left_join(subordination_df %>% select(document, DC_per_C, DC_per_S), by = "document") %>%
left_join(coordination_df %>% select(document, Coord_per_C, Coord_per_S), by = "document") %>%
left_join(nominal_df %>% select(document, CN_per_C, CN_per_S), by = "document")
all_measures %>%
knitr::kable(
digits = 2,
col.names = c("Document", "MLS", "C/S", "DC/C", "DC/S",
"Coord/C", "Coord/S", "CN/C", "CN/S")
)
all_measures <- mls_df %>%
left_join(clausal_density_df %>%
select(document, C_per_S), by = "document") %>%
left_join(subordination_df %>%
select(document, DC_per_C, DC_per_S), by = "document") %>%
left_join(coordination_df %>%
select(document, Coord_per_C, Coord_per_S), by = "document") %>%
left_join(nominal_df %>%
select(document, CN_per_C, CN_per_S), by = "document")
all_measures %>%
knitr::kable(
digits = 2,
col.names = c(
"Document",
"MLS",
"C/S",
"DC/C",
"DC/S",
"Coord/C",
"Coord/S",
"CN/C",
"CN/S")
)
syntax_long <- all_measures %>%  # ← Added %>%
pivot_longer(
cols = -document,
names_to = "Measure",
values_to = "Value"
) %>%
mutate(
Category = case_when(
Measure == "MLS" ~ "Sentence Length",
Measure == "C_per_S" ~ "Clausal Density",
Measure %in% c("DC_per_C", "DC_per_S") ~ "Subordination",
Measure %in% c("Coord_per_C", "Coord_per_S") ~ "Coordination",
Measure %in% c("CN_per_C", "CN_per_S") ~ "Phrasal Complexity"
)
)
# Plot
ggplot(syntax_long, aes(x = Measure, y = Value, fill = document)) +
geom_col(position = "dodge", width = 0.7) +
facet_wrap(~Category, scales = "free", ncol = 2) +
scale_fill_brewer(palette = "Set2") +
labs(
title = "Syntactic Complexity: Complete Profile",
subtitle = "Comparing multiple dimensions of syntactic complexity",
x = NULL,
y = "Value",
fill = "Document"
) +
theme_minimal(base_size = 12) +
theme(
axis.text.x = element_text(angle = 45, hjust = 1),
legend.position = "top"
)
